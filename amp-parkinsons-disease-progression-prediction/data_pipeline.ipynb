{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import the packages needed to run'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_preprocessing():\n",
    "    def __init__(self,df):\n",
    "        self.df =df\n",
    "\n",
    "    def data_pipeline(self):\n",
    "        \"\"\"\n",
    "        This function takes a dataset and identifies string or object data types which need to be encoded,\n",
    "        and leaves the other data types which are numerical as is.\n",
    "        \"\"\"\n",
    "        df=self.df\n",
    "        \n",
    "        # Identify string or object data types\n",
    "        cat_vars = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "        # If there are no string or object data types, print a message and return the original dataframe\n",
    "        if len(cat_vars) == 0:\n",
    "            print('No string or object data types found.')\n",
    "            return df\n",
    "\n",
    "        # If there are string or object data types, encode them using LabelEncoder or OneHotEncoder\n",
    "        for col in cat_vars:\n",
    "            # Check if column has a unique identifier\n",
    "            if len(df[col].unique()) == df[col].count():\n",
    "                print(f\"Skipping column {col} as it has a unique identifier.\")\n",
    "                continue\n",
    "            if len(df[col].unique()) > 2:\n",
    "                # If number of unique values is greater than 2, use one-hot encoding\n",
    "                ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                encoded = ohe.fit_transform(df[[col]].fillna('Unknown'))\n",
    "                new_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0]]\n",
    "                df[new_cols] = encoded\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            else:\n",
    "                # If number of unique values is 2 or less, use label encoding\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].fillna('Unknown'))\n",
    "\n",
    "        # Return the processed dataframe\n",
    "        return df\n",
    "\n",
    "\n",
    "    def nan_pipeline(self):\n",
    "        \"\"\"\n",
    "        This function takes a dataset and decides how to handle NaN values without affecting the statistical \n",
    "        properties too much, and gives a suggested output with a statement and the new dataframe.\n",
    "        \"\"\"\n",
    "        df=self.data_pipeline()\n",
    "\n",
    "        \n",
    "        # Identify columns with NaN values\n",
    "        nan_cols = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "        # If there are no NaN values, print a message and return the original dataframe\n",
    "        if len(nan_cols) == 0:\n",
    "            print('No NaN values found.')\n",
    "            return df\n",
    "\n",
    "        # If there are NaN values, impute them using IterativeImputer\n",
    "        imputer = IterativeImputer(max_iter=100, random_state=342)\n",
    "        imputed = imputer.fit_transform(df)\n",
    "        df_imputed = pd.DataFrame(imputed, columns=df.columns)\n",
    "\n",
    "        # Check for normality of each variable before and after imputation\n",
    "        normality_tests_before = []\n",
    "        for col in df.columns:\n",
    "            # Check if column has numeric data type\n",
    "            if not np.issubdtype(df[col].dtype, np.number):\n",
    "                continue\n",
    "            normal_test, p = normaltest(df[col].dropna())\n",
    "            normality_tests_before.append((col, normal_test, p))\n",
    "        normality_df_before = pd.DataFrame(normality_tests_before, columns=['column', 'normal_test_before', 'p_value_before'])\n",
    "\n",
    "        normality_tests_after = []\n",
    "        for col in df.columns:\n",
    "            # Check if column has numeric data type\n",
    "            if not np.issubdtype(df[col].dtype, np.number):\n",
    "                continue\n",
    "            normal_test, p = normaltest(df_imputed[col].dropna())\n",
    "            normality_tests_after.append((col, normal_test, p))\n",
    "        normality_df_after = pd.DataFrame(normality_tests_after, columns=['column', 'normal_test_after', 'p_value_after'])\n",
    "\n",
    "        # Calculate the difference in normality before and after imputation\n",
    "        normality_df = pd.merge(normality_df_before, normality_df_after, on='column')\n",
    "        normality_df['normal_test_diff'] = normality_df['normal_test_before'] - normality_df['normal_test_after']\n",
    "\n",
    "        # Suggest the best course of action based on the difference in normality before and after imputation\n",
    "        if (normality_df['normal_test_diff'] < -1000).any():\n",
    "            print('Imputed NaN values have greatly affected normality. Consider a different approach.')\n",
    "        elif (normality_df['normal_test_diff'] < 0).any():\n",
    "            print('Imputed NaN values have slightly affected normality. Proceed with caution.')\n",
    "        else:\n",
    "            print('Imputed NaN values have not affected normality significantly.')\n",
    "\n",
    "        # Return the imputed dataframe\n",
    "        return df_imputed\n",
    "\n",
    "\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        This function takes a dataset and determines the best way to handle missing values based on multiple tests.\n",
    "        \"\"\"\n",
    "        \n",
    "        df=self.nan_pipeline()\n",
    "\n",
    "\n",
    "        # Check for missing values\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print('No missing values found.')\n",
    "            return df\n",
    "\n",
    "        # Check for binary columns and replace missing values with the mode for binary columns\n",
    "        binary_vars = []\n",
    "        for col in df.columns:\n",
    "            if df[col].nunique() == 2:\n",
    "                binary_vars.append(col)\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "                df[col] = df[col].astype('category').cat.codes.astype('int8')\n",
    "        if binary_vars:\n",
    "            print(f\"Binary variables replaced NaNs with mode: {binary_vars}\")\n",
    "\n",
    "        # Check for string columns with ID-like values and leave them as is\n",
    "        id_vars = []\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            if df[col].apply(lambda x: x.isnumeric() or x.isdigit()).all():\n",
    "                id_vars.append(col)\n",
    "        if id_vars:\n",
    "            print(f\"Columns with unique identifiers: {id_vars}\")\n",
    "\n",
    "        # Check for normality of each variable\n",
    "        normality_tests = []\n",
    "        for col in df.columns:\n",
    "            # Check if column has numeric data type\n",
    "            if not np.issubdtype(df[col].dtype, np.number):\n",
    "                continue\n",
    "            normal_test, p = normaltest(df[col].dropna())\n",
    "            normality_tests.append((col, normal_test, p))\n",
    "        normality_df = pd.DataFrame(normality_tests, columns=['column', 'normal_test', 'p_value'])\n",
    "\n",
    "        # Identify variables with non-normal distributions\n",
    "        non_normal_vars = normality_df[normality_df['p_value'] < 0.05]['column']\n",
    "\n",
    "        # If all variables are normal, replace missing values with mean\n",
    "        if len(non_normal_vars) == 0:\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "            print('Missing values replaced with mean for all variables.')\n",
    "        else:\n",
    "            # Replace missing values with median for non-normal variables\n",
    "            if len(non_normal_vars) > 0:\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                df[non_normal_vars] = imputer.fit_transform(df[non_normal_vars])\n",
    "                print('Missing values replaced with median for non-normal variables:', non_normal_vars.tolist())\n",
    "\n",
    "            # Replace missing values with mode for categorical variables\n",
    "            cat_vars = df.select_dtypes(include=['category']).columns.tolist()\n",
    "            if len(cat_vars) > 0:\n",
    "                imputer = SimpleImputer(strategy='most_frequent')\n",
    "                df[cat_vars] = imputer.fit_transform(df[cat_vars])\n",
    "                print('Missing values replaced with mode for categorical variables:', cat_vars)\n",
    "\n",
    "        # Return the processed dataframe\n",
    "        return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
